{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7fcUFYHjk1R"
      },
      "source": [
        "# Handling multiple sequences (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLUGKEBWjk1W"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aRo1PObYjk1X",
        "outputId": "da051511-2ffe-4910-be40-831994910e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (5.29.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries: Transformers, Datasets, and Evaluate\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpicwPpwjk1Z",
        "outputId": "c9f7f80c-101a-4d42-905f-8822736e5421"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ],
      "source": [
        "# Import libraries and load a pre-trained model and tokenizer\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor([ids])\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbGsynVjjk1b",
        "outputId": "72f7ece8-fa2c-4976-d2e8-17a152a2d162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102]])\n"
          ]
        }
      ],
      "source": [
        "# Tokenize a sequence and return PyTorch tensors\n",
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYoa3aXtjk1c",
        "outputId": "7e9c6dad-5a7f-4be0-d3dd-cb140ead776c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Process a single sequence through the model\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcyWw8CWjk1f",
        "outputId": "709d9a76-57b5-423d-e4fd-ec52be469166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for sequence1_ids: tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
            "Logits for sequence2_ids: tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
            "Logits for batched_ids without attention mask: tensor([[ 1.5694, -1.3895],\n",
            "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate processing sequences of different lengths without padding\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(\"Logits for sequence1_ids:\", model(torch.tensor(sequence1_ids)).logits)\n",
        "print(\"Logits for sequence2_ids:\", model(torch.tensor(sequence2_ids)).logits)\n",
        "print(\"Logits for batched_ids without attention mask:\", model(torch.tensor(batched_ids)).logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxoJpPAVjk1h",
        "outputId": "7037426b-8fb2-4c61-ab8f-90747edc81b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for batched_ids with attention mask: tensor([[ 1.5694, -1.3895],\n",
            "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Process batched sequences with attention mask\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(\"Logits for batched_ids with attention mask:\", outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom function to add padding and create attention mask\n",
        "def add_padding(pad_token,input_ids):\n",
        "  max_len = len(max(input_ids, key=len))\n",
        "  attention_mask=[]\n",
        "  for idx,seq in enumerate(input_ids):\n",
        "    seq_len=len(seq)\n",
        "    seq_len_diff = max_len - seq_len\n",
        "\n",
        "    if seq_len_diff > 0:\n",
        "      input_ids[idx].extend([pad_token] * seq_len_diff)\n",
        "      attention_mask.append([1] * seq_len + [0]*seq_len_diff)\n",
        "    else:\n",
        "      attention_mask.append([1] * seq_len)\n",
        "\n",
        "  return (input_ids,attention_mask)"
      ],
      "metadata": {
        "id": "xoVl0x-Q0yCt"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare raw input sentences and tokenize them\n",
        "raw_inputs=[\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "\n",
        "\n",
        "input_ids2=[]\n",
        "for sequence in raw_inputs:\n",
        "  seq_tokens=tokenizer.tokenize(sequence)\n",
        "  seq_tokens.insert(0,\"[CLS]\")\n",
        "  seq_tokens.append(\"[SEP]\")\n",
        "  seq_ids=tokenizer.convert_tokens_to_ids(seq_tokens)\n",
        "  input_ids2.append(seq_ids)\n",
        "\n",
        "tokenizer.decode(input_ids2[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gzNFGLypHvqc",
        "outputId": "763f00a0-58bc-4234-8267-6a3870f7f239"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[CLS] i've been waiting for a huggingface course my whole life. [SEP]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process individual sequences through the model (corrected)\n",
        "seq1_output=model(torch.tensor(input_ids2[0]).unsqueeze(0))\n",
        "seq2_output=model(torch.tensor(input_ids2[1]).unsqueeze(0))\n",
        "\n",
        "print(\"Logits for sequence 1:\", seq1_output.logits)\n",
        "print(\"Logits for sequence 2:\", seq2_output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3tLVdCutB0V",
        "outputId": "025af521-5cee-4436-b04f-bec3f426a8b0"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for sequence 1: tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>)\n",
            "Logits for sequence 2: tensor([[ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process batched sequences with custom padding and attention mask\n",
        "padded_inputs,attention_mask=add_padding(tokenizer.pad_token_id,input_ids2)\n",
        "print(attention_mask)\n",
        "batch_seq_output = model(torch.tensor(padded_inputs), attention_mask=torch.tensor(attention_mask))\n",
        "print(batch_seq_output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcsfjNX8HqRr",
        "outputId": "3d5d54d8-a6fa-497c-9303-950f88950714"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "tensor([[-1.5607,  1.6123],\n",
            "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "F5YJEXiijk1i",
        "outputId": "6e7e2ac8-cf6e-4e6f-ee18-a8c256cd8094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 510\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I hate this so much!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "# Get the maximum sequence length for a single sentence from the tokenizer\n",
        "max_sequence_length = tokenizer.max_len_single_sentence\n",
        "print(\"Maximum sequence length:\", max_sequence_length)\n",
        "\n",
        "# Truncate a sequence to the maximum length (example)\n",
        "sequence = sequence[:max_sequence_length]\n",
        "sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a924edf4"
      },
      "source": [
        "*Note: This notebook is based on content from the Hugging Face LLM Course.*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Handling multiple sequences (PyTorch)",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}